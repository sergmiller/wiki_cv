\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[mathcal]{euscript}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[bottom=30mm,top=30mm,right=30mm,left=30mm,headsep=0cm,nofoot]{geometry}

\title{Wasserstein GAN}
\author{Сергей Миллер}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

\tableofcontents{}

\section{Введение}

В данном докладе будет рассмотрена задача генерации изображений из распределения заданного выборкой(то есть набором изображений-примеров). Оказывается, что достаточно распространенной ситуацией является расположение изображений из целевого распределения на некотором низкоразмерном многообразии в общем пространстве признаков(пикселей изображения). В таких случаях методы, оптимизирующие правдоподобие параметризованного распределения:

$$ \max_{\theta} \mathbf{E}_X \log P_{\theta}(x)$$

не имеют даже теоретических гарантий сходимости, и на практике, для преодаления этих ограничений приходится использовать зашумление компонент изображения(или иные техники). Новый подход к генеративным моделям предлагает специальную функцию потерь, которая не требует зашумления или иной предобработки, обладает меньшей чувствительностью к изменениям гиперпараметров, а также подвержена в меньшей степени взрывам и испарению градиентов(vanishing gradients).
Большая часть информации взята из статей \cite{wgan} и \cite{wgan-improvement}.

\section{Метрика The Earth-Mover(EM) distance или Wasserstein-1}

Ключевой идеей WGAN является использование специальной метрики (в пространстве распределений):

$$W(P_{r}, P_{g}) = \inf_{\gamma \sim \Pi(P_{r}, P_{g})} \mathbf{E}_{(x,y) \sim \gamma} |x - y|$$

где $\Pi(P_{r}, P_{g})$ - набор всех возможных распределений, маргинальными распределениями которых являются $P_{r}$ и $P_{g}$. Метрика имеет смысл оптимальной стоимости переноса массы из $x$ в $y$, для того, чтобы превратить распределение $P_{r}$ в $P_{g}$.

В оригинальной статье \cite{wgan} доказывается, что сходимость по EM-distance равносильна слабой сходимости(по распределению) в отличие от дивергенции Кульбака-Лейбнера, которая к тому же достаточно часто может быть неопределена в окресности искомого распределения.

\section{Оптимизация EM-distance}

Так как оптимизация EM-distance является достаточно трудной задачей, на практике решается в следующем виде: распределение $P_{\theta}$ задается параметризованной функцией $g_\theta$ от случайной величины $z \sim P_z$ (с известным распределением).

После такой параметризации использование следующих тождеств позволяет получить простой алгоритм оптимизации EM-distance:

$$W(P_r, P_{\theta}) = \sup_{||f||_L \leq 1} (\mathbf{E}_{x\sim P_r}f(x) - \mathbf{E}_{y\sim P_{\theta}}f(y))$$  \tag{1}

$$\nabla W(P_r, P_{\theta}) = -\mathbf{E}_{z \sim p(z)} \nabla_\theta f^*(g_{\theta}(z))$$ \tag{2}


где $||.||_L$ - Липшицева норма, $P_r$ - распределение реальных данных, а $f^* - \arg\max_f$ из тождества \eqref{1}.


На практике это означает, что мы можем, используя например нейронные сети, задать $g_\theta$ и$f_w$, поочередно выполняя шаги градиентного спуска(соответствующий равенствам \eqref{1}, \eqref{2}), пока $g_\theta$ не приблизится достаточно $P_\theta$ к  $P_r$, а $f_w$ к $f^*$.

\section{Алгоритм Wasserstein GAN}

 В итоге алгоритм в каноничном виде выглядит следующим образом:
 
\begin{figure}[h!]
\centering
\includegraphics[width=14cm]{wgan.png}
\end{figure}




Стоит заметить, что важной частью реализации является искуственное ограничение всех весов сети дискриминатора $f_w$ в диапазоне $[-c,c]$, для соблюдения условия Липшицевости оптимизируемой функции, чего хотелось бы избежать (более слабые ограничения в процессе обучения рассматривают в \cite{wgan-improvement}).

\newpage 

\section{Генерация изображений с помощью WGAN}

Рассмотрим сгенерированные примеры изображений для WGAN и классической схемой генеративной сети в задаче генерации интерьера комнаты (в обоих вариантах используются общие генератор и дискриминатор):




\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}

  \includegraphics[width=8.8cm]{GAN_example.png}
  \caption{GAN}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
  \includegraphics[width=9cm]{WGAN_example.png}
  \caption{WGAN}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

Несмотря на то, что по качеству изображения почти не отличимы, видно что классический GAN генерирует чуть-менее реалистичные интерьеры (в том плане, что предметы расположены относительно друг друга непривычным образом).

\caption{Wasserstein GAN: Gradient Penalty}

Слабым местом алгоритма обучения является искуственное загрубление весов сети дискриминатора для форсирования липшицевости. Более того, в \cite{wgan-improvement} показано, что веса критика(дискриминатора) очень быстро насыщаются, и функция достигает предельной константы, не сумев хорошо приблизить искомое распределение.

WGAN-GP лишен этого недостатка, для этого используется модифицированная функия потерь:

$$ L(\theta, w) = \mathbf{E}_{y\sim P_{\theta}}f_w(y) - \mathbf{E}_{x\sim P_r}f_w(x) + \lambda \mathbf{E}_{x \sim \frac{P_r+P_\theta}{2}} (||\nabla_x f_w(x)||_2 - 1)^2$$ \tag{3}

Смысл такой функции потерь в начислении дополнительного штрафа за отклонение нормы градиентов функции $f$ от 1, что в каком-то смысле ограничивает скорость роста функции $f$
В \cite{wgan-improvement} доказывается, что оптимизация такой функции потерь критика для WGAN также гарантирует липшицевость $f_w$ но нет потери выученной информации в весах сети.

\newpage


\section{WGAN vs WGAN-GP}


В задаче генерации интерьеров комнат качество сгенерированных изображений на порядок выше обычного WGAN, а также видно меньшее количество артефактов:

 
\begin{figure}[h]
\centering
\includegraphics[width=10cm]{wgan-gp.png}
\caption{WGAN-GP (использована та же архитектура генератора и критика, что и в предыдущих примерах)}
\end{figure}

\newpage

Также стоит отметить высокую стабильность при обучении WGAN-GP в различных конфигурациях архитектур генератора и критика по отношению к WGAN (а также DCGAN и LSGAN):

\begin{figure}[h]
\centering
\includegraphics[width=15cm]{comparison.png}
\end{figure}



\begin{thebibliography}{1}


\bibitem{wgan}
Martin Arjovsky, Soumith Chintala, Léon Bottou {\em  Wasserstein GAN}

\bibitem{wgan-improvement} Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville
{\em Improved Training of Wasserstein GANs}

\end{thebibliography}

\end{document}

